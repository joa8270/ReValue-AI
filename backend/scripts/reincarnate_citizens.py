
import os
import json
import asyncio
import logging
from dotenv import load_dotenv

# è¨­å®šæ—¥èªŒ (Move to top)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Load .env from backend root
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
load_dotenv(os.path.join(BASE_DIR, '.env'))

from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor

# å˜—è©¦å°å…¥ CrewAIï¼Œå¦‚æœæ²’æœ‰å®‰è£å‰‡å ±éŒ¯æç¤º

# å˜—è©¦å°å…¥ CrewAIï¼Œå¦‚æœæ²’æœ‰å®‰è£å‰‡ä½¿ç”¨ Fallback
HAS_CREWAI = False
try:
    from crewai import Agent, Task, Crew, Process
    # from langchain_openai import ChatOpenAI # Deprecated
    from langchain_google_genai import ChatGoogleGenerativeAI
    HAS_CREWAI = True
except ImportError:
    logger.warning("âš ï¸ CrewAI æœªå®‰è£æˆ–å®‰è£å¤±æ•—ï¼Œå°‡ä½¿ç”¨ Lightweight Fallback æ¨¡å¼åŸ·è¡Œä»»å‹™ã€‚")
    # å®šç¾© Fallback é¡åˆ¥ä»¥æ¨¡æ“¬ CrewAI ä»‹é¢
    class Agent:
        def __init__(self, role, goal, backstory, verbose=False, allow_delegation=False, llm=None):
            self.role = role
            self.goal = goal
            self.backstory = backstory
            self.llm = llm
        
        def execute_task(self, task_desc):
            # ç°¡å–®çš„ä½¿ç”¨ LLM åŸ·è¡Œ
            system_prompt = f"You are {self.role}. {self.backstory}\nYour goal is: {self.goal}"
            
            # Inject Schema for Data Engineer
            if "JSON" in task_desc or "json" in task_desc:
                schema_json = """
{
    "name_tw": "Generated Taiwanese Name (Traditional Chinese)",
    "US": {
        "name": "English Name",
        "city": "US City",
        "job": "US Job Title",
        "pain": "US Pain Point"
    },
    "CN": {
        "name": "Chinese Name",
        "city": "CN City",
        "job": "CN Job Title",
        "pain": "CN Pain Point"
    }
}
"""
                system_prompt += f"\nIMPORTANT: You must output ONLY valid JSON matching this structure exactly:\n{schema_json}\nDo not include root keys like 'agent_id' or 'data'. The root must have keys 'name_tw', 'US' and 'CN'."

            messages = [
                ("system", system_prompt),
                ("human", task_desc)
            ]
            if self.llm:
                content = self.llm.invoke(messages).content
                # Aggressive cleanup common in Gemini responses
                content = content.replace("```json", "").replace("```", "").strip()
                return content
            return "Simulated Output"

    class Task:
        def __init__(self, description, expected_output, agent, output_pydantic=None):
            self.description = description
            self.expected_output = expected_output
            self.agent = agent
            self.output_pydantic = output_pydantic
            self.output = None

    class Process:
        sequential = "sequential"

    class Crew:
        def __init__(self, agents, tasks, process=Process.sequential, verbose=False):
            self.agents = agents
            self.tasks = tasks
            self.verbose = verbose

        def kickoff(self):
            logger.info("ğŸš€ Starting Crew (Fallback Mode)")
            context = ""
            for task in self.tasks:
                logger.info(f"ğŸ‘‰ Agent {task.agent.role} working on task...")
                # å°‡ä¸Šä¸€å€‹ä»»å‹™çš„çµæœä½œç‚º Context
                full_transcription = f"{task.description}\n\nContext from previous steps:\n{context}"
                result = task.agent.execute_task(full_transcription)
                
                # å¦‚æœéœ€è¦ Pydantic è§£æ
                if task.output_pydantic:
                    try:
                        # å˜—è©¦è§£æ JSON
                        clean_json = result.replace("```json", "").replace("```", "").strip()
                        # é€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›å¯èƒ½éœ€è¦é‡è©¦
                        import json
                        data_dict = json.loads(clean_json)
                        # é©—è­‰çµæ§‹ (ç°¡å–®æª¢æŸ¥)
                        # é©—è­‰çµæ§‹ (ç°¡å–®æª¢æŸ¥)
                        # result = task.output_pydantic(**data_dict) # Direct unpacking might fail if keys mismatch
                        
                        # Safer construction manually mapping fields to avoid validation errors
                        result = task.output_pydantic(
                            name_tw=data_dict.get("name_tw", "Unknown"),
                            US=data_dict.get("US", {}),
                            CN=data_dict.get("CN", {})
                        )
                    except Exception as e:
                        logger.error(f"JSON Parsing failed in fallback: {e}")
                        # å›å‚³ç©ºç‰©ä»¶é¿å…å´©æ½°
                        result = task.output_pydantic(name_tw="Unknown", US={"name":"", "city":"", "job":"", "pain":""}, CN={"name":"", "city":"", "job":"", "pain":""})
                
                task.output = result
                context += f"\nOutput of {task.agent.role}: {result}"
            
            return self.tasks[-1].output

try:
    from langchain_google_genai import ChatGoogleGenerativeAI
except ImportError:
    print("âŒ éŒ¯èª¤: è«‹å…ˆå®‰è£ä¾è³´åº«: pip install langchain-google-genai")
    exit(1)

from pydantic import BaseModel, Field


# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ==========================================
# 1. å®šç¾©è³‡æ–™çµæ§‹ (Pydantic Models)
# ==========================================

class ProfileData(BaseModel):
    name: str = Field(description="è©²åœ‹å®¶èº«åˆ†çš„å§“å Identity Name")
    city: str = Field(description="å±…ä½åŸå¸‚ (e.g., Austin, Shenzhen)")
    job: str = Field(description="è·æ¥­é ­éŠœ (e.g., Senior Engineer)")
    pain: str = Field(description="è©²éšç´šçš„æ ¸å¿ƒç„¦æ…® (e.g., H1B Visa, 35æ­²å„ªåŒ–)")

class GlobalIdentity(BaseModel):
    name_tw: str = Field(description="[NEW] ç‚ºæ­¤å¸‚æ°‘ç”Ÿæˆçš„çœŸå¯¦å°ç£å§“å (ç¹é«”ä¸­æ–‡, 1990-2000å‡ºç”Ÿé¢¨æ ¼)")
    US: ProfileData = Field(description="ç¾åœ‹èº«åˆ†è³‡æ–™")
    CN: ProfileData = Field(description="ä¸­åœ‹èº«åˆ†è³‡æ–™")

# ==========================================
# 2. å®šç¾© Agent èˆ‡ Task 
# ==========================================

def create_reincarnation_crew(citizen: Dict) -> Crew:
    """
    ç‚ºå–®ä¸€ä½å¸‚æ°‘å»ºç«‹è½‰ç”Ÿåœ˜éšŠ
    """
    
    # æå–é—œéµè³‡è¨Š
    citizen_id = citizen.get('id')
    tw_profile = f"ç•¶å‰ä»£è™Ÿ: {citizen.get('name')}, å±…ä½: {citizen.get('city')}, è·ä½: {citizen.get('job')}, å¹´é½¡: {citizen.get('age')}"
    bazi = citizen.get('bazi', 'Unknown')
    bazi_summary = citizen.get('bazi_summary', '')

    # è¨­å®š Gemini LLM (Gemini 2.5 Pro)
    gemini_llm = ChatGoogleGenerativeAI(
        model="models/gemini-2.5-pro",
        verbose=True,
        temperature=0.8,
        google_api_key=os.getenv("GOOGLE_API_KEY")
    )

    # --- Agent 1: æ–‡åŒ–äººé¡å­¸å®¶ ---
    anthropologist = Agent(
        role='Cultural Anthropologist (æ–‡åŒ–äººé¡å­¸å®¶)',
        goal='ç‚ºå¸‚æ°‘è³¦äºˆçœŸå¯¦å§“åï¼Œä¸¦æ˜ å°„å‡ºç¾ä¸­å¹³è¡Œèº«åˆ†',
        backstory="""ä½ æ˜¯ä¸€ä½ç²¾é€šæ±äºèˆ‡åŒ—ç¾ç¤¾æœƒçµæ§‹çš„æ–‡åŒ–äººé¡å­¸å®¶ã€‚
        ä½ çš„ä»»å‹™æœ‰å…©å€‹ï¼š
        1. **ç‚ºä»£è™Ÿå¸‚æ°‘å‘½å**ï¼šç›®å‰çš„å¸‚æ°‘åªæœ‰ä»£è™Ÿ (Citizen_xxxx)ã€‚è«‹æ ¹æ“šä»–çš„ã€Œå…«å­—æ€§æ ¼ã€èˆ‡ã€Œè·æ¥­ã€ï¼Œç‚ºä»–å–ä¸€å€‹**çœŸå¯¦ã€æ¥åœ°æ°£çš„å°ç£åå­—** (ç¹é«”ä¸­æ–‡)ã€‚
           - å¹´é½¡è¨­å®šç‚º 1990-2000 å¹´å‡ºç”Ÿ (ç´„ 25-35 æ­²)ã€‚
           - åå­—è¦ç¬¦åˆè©²ä¸–ä»£çš„å‘½åç¿’æ…£ (å¦‚ï¼šé›…å©·, å† å®‡, æ€¡å›, æ‰¿æ©, ç­±æ¶µ ç­‰ï¼Œæˆ–æ›´ç¾ä»£çš„)ã€‚
           - åå­—æ°£è³ªéœ€ç¬¦åˆå…¶å‘½ç†æ ¼å±€ (ä¾‹å¦‚ã€Œä¸ƒæ®ºæ ¼ã€å¯èƒ½å« å¿—è±ªï¼Œã€Œé£Ÿç¥æ ¼ã€å¯èƒ½å« ä½³è–‡)ã€‚
        
        2. **å¹³è¡Œå®‡å®™è½‰ç”Ÿ**ï¼šæ¨å°å‡ºä»–åœ¨ç¾åœ‹ (US) èˆ‡ä¸­åœ‹ (CN) çš„å°æ‡‰èº«åˆ†èˆ‡ç„¦æ…®ã€‚""",
        verbose=True,
        allow_delegation=False,
        llm=gemini_llm 
    )

    # --- Agent 2: è³‡æ–™å·¥ç¨‹å¸« ---
    data_engineer = Agent(
        role='Data Engineer (è³‡æ–™å·¥ç¨‹å¸«)',
        goal='å°‡æ•˜äº‹æ€§çš„èº«åˆ†æè¿°è½‰åŒ–ç‚ºåš´æ ¼çš„ JSON æ ¼å¼',
        backstory="""ä½ æ˜¯ä¸€ä½å°è³‡æ–™çµæ§‹æœ‰æ½”ç™–çš„å·¥ç¨‹å¸«ã€‚è² è²¬è¼¸å‡º JSONã€‚""",
        verbose=True,
        allow_delegation=False,
        llm=gemini_llm 
    )

    # --- Task 1: èº«åˆ†æ˜ å°„ ---
    mapping_task = Task(
        description=f"""
        åˆ†æä»¥ä¸‹å°ç£å¸‚æ°‘è³‡æ–™ï¼š
        [ID]: {citizen_id}
        [Profile]: {tw_profile}
        [Bazi]: {bazi}

        ä»»å‹™ A: å‘½å (Naming)
        è«‹ç‚ºä»–å–ä¸€å€‹çœŸå¯¦çš„å°ç£å§“å (name_tw)ã€‚

        ä»»å‹™ B: è½‰ç”Ÿ (Reincarnation)
        1. **US Identity**:
           - è‹±æ–‡å§“å (å¯èˆ‡ä¸­æ–‡åè«§éŸ³æˆ–å®Œå…¨ä¸åŒ)ã€‚
           - åŸå¸‚ (ç”¢æ¥­èšè½)ã€‚
           - è·æ¥­ (å°æ‡‰è³‡æ­·)ã€‚
           - **Pain Point**: çœŸå¯¦çš„ç¾åœ‹è·å ´/ç”Ÿæ´»ç„¦æ…®ã€‚
        
        2. **CN Identity**:
           - ç°¡é«”ä¸­æ–‡å§“åã€‚
           - ä¸€ç·š/æ–°ä¸€ç·šåŸå¸‚ã€‚
           - è·æ¥­ (å¤§å» è·ç´š P6/P7 ç­‰)ã€‚
           - **Pain Point**: å…§æ²ã€æˆ¿è²¸ã€35æ­²å„ªåŒ–ç­‰çœŸå¯¦ç„¦æ…®ã€‚
        """,
        expected_output="ä¸€ä»½åŒ…å« å°ç£æ–°å§“åã€US èº«åˆ†ã€CN èº«åˆ† çš„å®Œæ•´åˆ†æã€‚",
        agent=anthropologist
    )

    # --- Task 2: æ ¼å¼åŒ– ---
    formatting_task = Task(
        description="å°‡åˆ†æå ±å‘Šè½‰æ›ç‚º JSONï¼Œå¿…é ˆåŒ…å« name_tw, US, CN æ¬„ä½ã€‚",
        expected_output="JSON object matching GlobalIdentity schema.",
        agent=data_engineer,
        output_pydantic=GlobalIdentity
    )

    crew = Crew(
        agents=[anthropologist, data_engineer],
        tasks=[mapping_task, formatting_task],
        process=Process.sequential,
        verbose=True
    )

    return crew

# ==========================================
# 3. éåŒæ­¥è™•ç†é‚è¼¯
# ==========================================

async def process_single_citizen(semaphore: asyncio.Semaphore, citizen: Dict) -> Dict:
    async with semaphore:
        try:
            logger.info(f"ğŸš€ é–‹å§‹è½‰ç”Ÿ: {citizen.get('id')} ({citizen.get('bazi')})")
            print(f"DEBUG: Processing {citizen.get('id')}...")

            
            crew = create_reincarnation_crew(citizen)
            
            # Run Crew
            # Note: CrewAI kickoff is blocking.
            result = await asyncio.to_thread(crew.kickoff)
            
            # Parse Result
            if hasattr(result, 'dict'):
                parsed_data = result.dict()
            elif hasattr(result, 'model_dump'):
                 parsed_data = result.model_dump()
            elif isinstance(result, str):
                try:
                    clean_json = result.replace("```json", "").replace("```", "").strip()
                    parsed_data = json.loads(clean_json)
                except:
                    logger.error(f"âš ï¸ JSON Parse Error for {citizen.get('id')}")
                    logger.error(f"âš ï¸ JSON Parse Error for {citizen.get('id')}")
                    parsed_data = {"name_tw": "Unknown", "US": {}, "CN": {}}
            else:
                parsed_data = {}

            # Construct Record
            # å„ªå…ˆä½¿ç”¨ LLM ç”Ÿæˆçš„ name_twï¼Œå¦‚æœæ²’æœ‰å‰‡å›é€€åˆ°åŸå§‹ name (ä½†é€šå¸¸æ‡‰è©²è¦æœ‰)
            new_name = parsed_data.get('name_tw') 
            if not new_name or new_name == "Unknown":
                 new_name = citizen.get('name')
            
            final_record = {
                "id": citizen.get('id'),
                "name": new_name, # REPLACE ROOT NAME!
                "bazi": citizen.get('bazi'),
                "gender": citizen.get('gender'), # Preserve original gender
                "age": citizen.get('age'),
                "profiles": {
                    "TW": {
                        "name": new_name,
                        "city": citizen.get('city'),
                        "job": citizen.get('job'),
                        "pain": "æœªçŸ¥ (å¾…è£œå®Œ)" 
                    },
                    "US": parsed_data.get('US', {}),
                    "CN": parsed_data.get('CN', {})
                }
            }
            
            logger.info(f"âœ… è½‰ç”ŸæˆåŠŸ: {citizen.get('id')} -> {new_name}")
            return final_record

        except Exception as e:
            logger.error(f"âŒ è™•ç†å¤±æ•— {citizen.get('id')}: {str(e)}")
            import traceback
            traceback.print_exc()
            return None

import random

BAZI_TYPES = [
    "é£Ÿç¥æ ¼ (è¬›ç©¶é«”é©—)", "ä¸ƒæ®ºæ ¼ (è¡Œå‹•æ´¾)", "æ­£å®˜æ ¼ (å®ˆè¦çŸ©)", 
    "åè²¡æ ¼ (æ©Ÿéˆ)", "æ­£å°æ ¼ (ä»æ…ˆ)", "å‚·å®˜æ ¼ (å›é€†)", 
    "æ¯”è‚©æ ¼ (è‡ªæˆ‘)", "åŠ«è²¡æ ¼ (ç«¶çˆ­)"
]

MBTI_TYPES = ["INTJ", "INTP", "ENTJ", "ENTP", "INFJ", "INFP", "ENFJ", "ENFP", "ISTJ", "ISFJ", "ESTJ", "ESFJ", "ISTP", "ISFP", "ESTP", "ESFP"]

CITIES_TW = ["Taipei", "New Taipei", "Taichung", "Kaohsiung", "Hsinchu", "Tainan"]
JOBS_TW = ["Engineer", "Teacher", "Sales", "Designer", "PM", "Marketing", "Freelancer", "Student", "Civil Servant", "Doctor"]

def generate_raw_seeds(output_path: str, count: int = 1000):
    logger.info(f"ğŸŒ± Generating {count} Raw Seeds...")
    seeds = []
    for i in range(count):
        seed = {
            "id": f"Citizen_{i:04d}",
            "name": f"Citizen_{i:04d}", # Temp name
            "age": random.randint(22, 45),
            "gender": random.choice(["Male", "Female", "Non-binary"]),
            "city": random.choice(CITIES_TW),
            "job": random.choice(JOBS_TW),
            "bazi": random.choice(BAZI_TYPES),
            "mbti": random.choice(MBTI_TYPES),
            "bazi_summary": "Auto-generated seed."
        }
        seeds.append(seed)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(seeds, f, ensure_ascii=False, indent=2)
    
    logger.info(f"âœ… Raw seeds saved to {output_path}")
    return seeds

async def main():
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    DATA_FILE = os.path.join(BASE_DIR, 'data', 'citizens.json')
    OUTPUT_FILE = os.path.join(BASE_DIR, 'data', 'citizens_global_v2.json') # saving to v2

    citizens = []
    DO_DEBUG = False

    
    # 1. è®€å–èˆ‡è‡ªå‹•è£œç¨®
    need_seeding = False
    if not os.path.exists(DATA_FILE):
        need_seeding = True
    else:
        try:
            with open(DATA_FILE, 'r', encoding='utf-8') as f:
                citizens = json.load(f)
            if len(citizens) < 10:
                need_seeding = True
        except:
             need_seeding = True
    
    if need_seeding:
        logger.warning(f"âš ï¸ Source data invalid or empty (Count: {len(citizens)}). Triggering Auto-Seeding.")
        citizens = generate_raw_seeds(DATA_FILE, 1000)
        print("ğŸ”„ Generated 1,000 raw seeds because source was empty.")

    # 0. Clean Slate (User Request)
    if os.path.exists(OUTPUT_FILE):
        logger.warning(f"ğŸ§¹ Clearing existing output file: {OUTPUT_FILE}")
        os.remove(OUTPUT_FILE)

    logger.info(f"ğŸ”¥ Project REINCARNATION V2 Start. Total: {len(citizens)}")
    if DO_DEBUG:
        citizens = citizens[:2]
        print("[DEBUG] Processing only 2 citizens.")

    
    # Batch Processing with Incremental Save
    BATCH_SIZE = 10 # Save every 10
    MAX_CONCURRENCY = 5 # Moderate concurrency to avoid rate limits
    semaphore = asyncio.Semaphore(MAX_CONCURRENCY)
    
    completed_citizens = []
    total_processed = 0
    
    # Iterate in batches
    for i in range(0, len(citizens), BATCH_SIZE):
        batch = citizens[i : i + BATCH_SIZE]
        logger.info(f"âš¡ Processing Batch {i}-{i+len(batch)}...")
        
        tasks = [process_single_citizen(semaphore, c) for c in batch]
        results = await asyncio.gather(*tasks)
        
        valid_batch = [r for r in results if r is not None]
        completed_citizens.extend(valid_batch)
        total_processed += len(valid_batch)
        
        # INCREMENTAL SAVE
        logger.info(f"ğŸ’¾ Saving {total_processed} citizens to {OUTPUT_FILE}...")
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(completed_citizens, f, ensure_ascii=False, indent=2)

    logger.info(f"ğŸ‰ All Done! Generated {len(completed_citizens)} citizens.")

if __name__ == "__main__":
    asyncio.run(main())
